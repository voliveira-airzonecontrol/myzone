environment: prod

# MLFlow configuration section
mlflow:
  tracking_uri: "http://192.168.2.241:5000"

# Training configuration section
training:
  random_state: 42
  tfidf:
    processor: TfidfVectorizer
    model: KMeans
    # TfidfVectorizer hyperparameters
    min_df:
      - 0.01
      - 0.05
    max_df:
      - 0.95
      - 0.99
    # KMeans hyperparameters
    n_clusters:
      - 50
      - 150
      - 250
      - 350
      - 450

  doc2vec:
    processor: Doc2Vec
    model: KMeans
    # Doc2Vec hyperparameters
    dm:
      - 0
      #- 1
    vector_size:
      #- 150
      - 200
      #- 250
    epochs:
      #- 10
      - 20
      #- 30
    min_count:
      #- 1
      - 2
      #- 3
    sample:
      - 0
      #- 1e-3
    negative:
      - 5
    hs:
      - 0
      #- 1
    # KMeans hyperparameters
    n_clusters:
      #- 50
      #- 150
      #- 250
      #- 350
      - 450
      - 550

  conformal_prediction:
    alpha:
      - 0.2
      - 0.15
      - 0.1

  sentence_transformer:
    processor: SentenceTransformer
    model: KMeans
    # SentenceTransformer hyperparameters
    transformer_name: all-mpnet-base-v2
    # KMeans hyperparameters
    n_clusters:
      - 50
      - 150
      - 250
      - 350
      - 450

  BERT:
    test_size: 0.25
    cp_size: 0.5
    val_size: 0.15
    model: dtorber/bert-base-spanish-wwm-cased_K4
    features: text_to_analyse
    target: label
    random_state:
      - ${training.random_state}

  XGBoost:
    test_size: 0.3
    cp_size: 0.5
    model: XGBClassifier
    features: embedding
    target: label
    params_list:
      - device
      - learning_rate
      - max_depth
      - n_estimators
      - subsample
      - sampling_method
      - colsample_bytree
      #- scale_pos_weight
      - max_delta_step
      - tree_method
      - random_state

    cv: 3
    n_jobs: -1
    scoring: f1_micro

    # XGBClassifier hyperparameters#
    device:
      - "cuda"
    learning_rate:
      - 0.05
      # - 0.1
    max_depth:
      # - 50
      # - 75
      - 100
    n_estimators:
      # - 200
      - 300
    subsample:
      - 0.7
      - 0.8
    sampling_method:
      - "gradient_based"
    colsample_bytree:
      - 0.7
      - 0.8
    #scale_pos_weight:
      #- 95
      #- 98
    max_delta_step:
      - 0
      #- 1
    tree_method:
      - "hist"
    random_state:
      - ${training.random_state}

  SVC:
    test_size: 0.3
    cp_size: 0.5
    model: SVC
    features: embedding
    target: label
    params_list:
      - C
      - kernel
      - gamma
      - random_state

    cv: 3
    n_jobs: -1
    scoring: f1_micro

    # SVC hyperparameters
    C:
      - 0.01
      - 0.1
      - 1
      - 10
      - 100
      - 1000
    kernel:
      #- "linear"
      - "rbf"
    gamma:
      #- "scale"
      - "auto"
      - 0.1
      - 1
      - 10
      - 100
    random_state:
      - ${training.random_state}